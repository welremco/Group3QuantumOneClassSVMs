{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc664edf",
   "metadata": {},
   "source": [
    "ZZ Feature Map Algorithm 1 test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de16c4",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d659b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import BasicAer\n",
    "from qiskit import Aer, transpile\n",
    "\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.circuit.library import PauliFeatureMap\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit.circuit.library import StatePreparation\n",
    "\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "import qiskit_machine_learning.kernels\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from qiskit.providers.aer import AerError\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "algorithm_globals.random_seed = 0\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "\n",
    "from qiskit.algorithms.linear_solvers.numpy_linear_solver import NumPyLinearSolver\n",
    "from qiskit.algorithms.linear_solvers.hhl import HHL\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "from functools import reduce\n",
    "from sympy import Matrix\n",
    "from sympy import sqrt as special_sqrt\n",
    "from qiskit import *\n",
    "from qiskit.extensions import HamiltonianGate\n",
    "from qiskit.quantum_info import Operator\n",
    "\n",
    "dataset_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a5f750",
   "metadata": {},
   "source": [
    "Code used to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4925e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(13)\n",
    "# seed = 0\n",
    "# x, y = make_blobs(n_samples=190, centers=1, cluster_std=.1, center_box=(1.4, 1.4), random_state=seed)\n",
    "\n",
    "# out1x, out1y = make_blobs(n_samples=9, centers=1, cluster_std=.1, center_box=(1, 1), random_state=seed)\n",
    "# out2x, out2y = make_blobs(n_samples=11, centers=1, cluster_std=.1, center_box=(3.2, 0.01), random_state=seed)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# y[y == 0] = 1\n",
    "# out1y[out1y==0] = -1 \n",
    "# out2y[out2y==0] = -1 \n",
    "\n",
    "# x = np.append(x, out1x, axis = 0)\n",
    "# y = np.append(y, out1y, axis = 0)\n",
    "\n",
    "# x = np.append(x, out2x, axis = 0)\n",
    "# y = np.append(y, out2y, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(x[:,0], x[:,1])\n",
    "# plt.scatter(out1x[:,0], out1x[:,1])\n",
    "# plt.scatter(out2x[:,0], out2x[:,1])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626d6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second dataset: ZZFeatureMap ad_hoc data\n",
    "# adhoc_dimension = 2\n",
    "# train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "#     training_size=80,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.3,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=True,\n",
    "# )\n",
    "# train_features_out, train_labels_out, _, _ = ad_hoc_data(\n",
    "#     training_size=10,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.6,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "# # adhoc_total[adhoc_total == 0] = 1\n",
    "\n",
    "# # print(train_features)\n",
    "# # print(train_labels)\n",
    "\n",
    "# # Change labels for One-Class\n",
    "# train_labels_out[train_labels_out != -1] = -1\n",
    "\n",
    "# # Now we have to add the outliers\n",
    "# train_features = np.append(train_features, train_features_out, axis = 0)\n",
    "# train_labels = np.append(train_labels, train_labels_out, axis = 0)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.ylim(0, 2 * np.pi)\n",
    "# plt.xlim(0, 2 * np.pi)\n",
    "# plt.imshow(\n",
    "#     np.asmatrix(adhoc_total).T,\n",
    "#     interpolation=\"nearest\",\n",
    "#     origin=\"lower\",\n",
    "#     cmap=\"RdBu\",\n",
    "#     extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    "# )\n",
    "\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 0), 0],\n",
    "#     train_features[np.where(train_labels[:] == 0), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"b\",\n",
    "#     label=\"Training Label A\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 1), 0],\n",
    "#     train_features[np.where(train_labels[:] == 1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "#     label=\"Training Label B\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == -1), 0],\n",
    "#     train_features[np.where(train_labels[:] == -1), 1],\n",
    "#     marker=\"s\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"k\",\n",
    "#     label=\"Outliers\",\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "# plt.title(\"Ad hoc dataset for classification\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # We don't need the second label for one-class SVMs, so change 0 label to 1\n",
    "# train_labels[train_labels == 0] = 1\n",
    "\n",
    "\n",
    "# # add data to dataset_list\n",
    "# dataset_list.append([train_features,train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e8ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Third dataset, create blob and add zz feature map data to it\n",
    "# random.seed(13)\n",
    "# seed = 23\n",
    "# x, y = make_blobs(n_samples=190, centers=1, cluster_std=3, center_box=(3, 3), random_state=seed)\n",
    "# train_features_out2, train_labels_out2, _, _ = ad_hoc_data(\n",
    "#     training_size=10,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.5,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# y[y == 0] = 1\n",
    "# train_labels_out2[train_labels_out2 != -1] = -1 \n",
    "\n",
    "# # add outliers to data\n",
    "# x = np.append(x, train_features_out2, axis = 0)\n",
    "# y = np.append(y, train_labels_out2, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(x[:,0], x[:,1])\n",
    "# plt.scatter(\n",
    "#     train_features_out2[np.where(train_labels_out2[:] == -1), 0],\n",
    "#     train_features_out2[np.where(train_labels_out2[:] == -1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "#     label=\"Training Label B\",\n",
    "# )\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([x,y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f41d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adhoc_dimension = 2\n",
    "# train_features, train_labels, _,_ = ad_hoc_data(\n",
    "#     training_size=80,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.6,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "# x, y = make_blobs(n_samples=20, centers=3, cluster_std=2, center_box=(3, 3), random_state=111)\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# train_labels[train_labels == 0] = 1\n",
    "# y[y != -1] = -1 \n",
    "\n",
    "# # add outliers to data\n",
    "# train_features = np.append(train_features, x, axis = 0)\n",
    "# train_labels = np.append(train_labels, y, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 1), 0],\n",
    "#     train_features[np.where(train_labels[:] == 1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"b\",\n",
    "#     label=\"Training Label A\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     x[np.where(y[:] == -1), 0],\n",
    "#     x[np.where(y[:] == -1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "# )\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([train_features,train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a8ae",
   "metadata": {},
   "source": [
    "Save/Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d2a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as fileload:\n",
    "        file = pickle.load(fileload)\n",
    "    return file\n",
    "# save_object(dataset_list, \"datasets\")\n",
    "dataset_list = load_object(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21620054",
   "metadata": {},
   "source": [
    "Custom Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d45d51b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_map_func(x):\n",
    "    mapped = x[0] if len(x) == 1 else reduce(lambda m, n: m * n, x)\n",
    "    return mapped\n",
    "def feature_map_superfidel(x):\n",
    "    # as described in \n",
    "    # https://doi.org/10.1103/PhysRevA.97.042315\n",
    "    \n",
    "    # Qiskit currently doesn't natively support a square root function in a parameter expression\n",
    "    # So use sympy base to get the same effect\n",
    "    mapped = x[0] if len(x) == 1 else reduce(lambda m, n: m * n, \n",
    "                                             np.divide(x,(1-np.square(np.column_stack(x)).trace())._call(special_sqrt)))\n",
    "    return mapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e32a7",
   "metadata": {},
   "source": [
    "Quantum Function for OneClass (Algorithm 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58ba946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First algorithm, returns trained model\n",
    "def Algorithm1(X, y, reps=2, shots=1, outliers_fraction=20/210,\n",
    "               entanglement=\"linear\", num_features = 2, seed = 0, \n",
    "               supervised=False, feature_map_no = 1, data_map_no = 1) :\n",
    "    if feature_map_no == 1:\n",
    "        # Define ZZFeatureMap using inputs\n",
    "        if data_map_no == 1:\n",
    "             feature_map = ZZFeatureMap(feature_dimension = num_features, \n",
    "                                   reps = reps, entanglement=entanglement)\n",
    "        elif data_map_no == 2:\n",
    "            feature_map = ZZFeatureMap(feature_dimension = num_features, \n",
    "                                   reps = reps, entanglement=entanglement, \n",
    "                                       data_map_func = custom_data_map_func)\n",
    "        elif data_map_no == 3:\n",
    "            feature_map = ZZFeatureMap(feature_dimension = num_features, \n",
    "                                   reps = reps, entanglement=entanglement, \n",
    "                                       data_map_func = feature_map_superfidel)\n",
    "    elif feature_map_no == 2:\n",
    "        # Define ZZFeatureMap using inputs\n",
    "        feature_map = ZFeatureMap(feature_dimension = num_features, reps = reps, entanglement=entanglement)\n",
    "    elif feature_map_no == 3:\n",
    "        # Define ZZFeatureMap using inputs\n",
    "        feature_map = PauliFeatureMap(feature_dimension = num_features, reps = reps, entanglement=entanglement)\n",
    "    # Calculates probabilities of bit results from quantum circuits\n",
    "    sampler = Sampler()\n",
    "    # uses sampler to calculate state fidelity of 2 quantum circuits\n",
    "    fidelity = ComputeUncompute(sampler=sampler)\n",
    "    # Translates data with base state fidelity distance metric\n",
    "    kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "    # Kernel needs to be evaluated before going into the One-Class SVM\n",
    "    svm = OneClassSVM(kernel = kernel.evaluate, verbose=True, nu=outliers_fraction)\n",
    "    if supervised: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        # TODO save to Matrix\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='macro')))\n",
    "        print(\"F1 1: {}\".format(f1_score(y, y_pred, average='macro')))\n",
    "#         print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "#         print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "#         print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "#         print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "    else: \n",
    "        svm.fit(X)\n",
    "        y_pred = svm.predict(X)\n",
    "        #TODO save to matrix\n",
    "#         print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "        \n",
    "        print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='macro')))\n",
    "        print(\"F1 1: {}\".format(f1_score(y, y_pred, average='macro')))\n",
    "\n",
    "#         print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "#         print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "#         print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "#         print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0a968",
   "metadata": {},
   "source": [
    "Showcase of Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1bba916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For dataset: 1\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.60      0.60        20\n",
      "           1       0.96      0.96      0.96       190\n",
      "\n",
      "    accuracy                           0.92       210\n",
      "   macro avg       0.78      0.78      0.78       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n",
      "Accuracy: 0.9238095238095239\n",
      "Precision 1: 0.9578947368421052\n",
      "Precision -1: 0.6\n",
      "Recall 1: 0.9578947368421052\n",
      "Recall -1: 0.6\n",
      "F1 1: 0.9578947368421052\n",
      "F1 -1: 0.6\n",
      "\n",
      "For dataset: 2\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.09      0.20      0.13        20\n",
      "           1       0.88      0.76      0.81       160\n",
      "\n",
      "    accuracy                           0.69       180\n",
      "   macro avg       0.49      0.48      0.47       180\n",
      "weighted avg       0.80      0.69      0.74       180\n",
      "\n",
      "Accuracy: 0.6944444444444444\n",
      "Precision 1: 0.8832116788321168\n",
      "Precision -1: 0.09302325581395349\n",
      "Recall 1: 0.75625\n",
      "Recall -1: 0.2\n",
      "F1 1: 0.8148148148148148\n",
      "F1 -1: 0.12698412698412698\n",
      "\n",
      "For dataset: 3\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.16      0.50      0.24        20\n",
      "           1       0.93      0.72      0.81       190\n",
      "\n",
      "    accuracy                           0.70       210\n",
      "   macro avg       0.54      0.61      0.52       210\n",
      "weighted avg       0.86      0.70      0.76       210\n",
      "\n",
      "Accuracy: 0.6952380952380952\n",
      "Precision 1: 0.9315068493150684\n",
      "Precision -1: 0.15625\n",
      "Recall 1: 0.7157894736842105\n",
      "Recall -1: 0.5\n",
      "F1 1: 0.8095238095238095\n",
      "F1 -1: 0.23809523809523808\n",
      "\n",
      "For dataset: 4\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.11      0.20      0.15        20\n",
      "           1       0.89      0.81      0.85       160\n",
      "\n",
      "    accuracy                           0.74       180\n",
      "   macro avg       0.50      0.50      0.50       180\n",
      "weighted avg       0.80      0.74      0.77       180\n",
      "\n",
      "Accuracy: 0.7388888888888889\n",
      "Precision 1: 0.8896551724137931\n",
      "Precision -1: 0.11428571428571428\n",
      "Recall 1: 0.80625\n",
      "Recall -1: 0.2\n",
      "F1 1: 0.8459016393442622\n",
      "F1 -1: 0.14545454545454545\n"
     ]
    }
   ],
   "source": [
    "dataset_count = 0\n",
    "outliers_fraction=20/210\n",
    "entanglement_list = [\"full\", 'linear']\n",
    "data_map_list = []\n",
    "for X, y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print()\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    print()\n",
    "    print(\"Experiment with number of repetitions 1,2,3\")\n",
    "    for i in range(1,4):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, reps=i)\n",
    "    print()\n",
    "    print(\"Experiment with different Feature maps\")\n",
    "    for i in range(1,4):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, feature_map_no=i)\n",
    "    print()\n",
    "    print(\"Experiment with different entanglements\")\n",
    "    for i in entanglement_list:\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, entanglement=i)\n",
    "    print()\n",
    "    print(\"Experiment with different data_maps\")\n",
    "    for i in range(1,4):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, data_map_no)\n",
    "    print()\n",
    "    print(\"Experiment with different shots\")\n",
    "    for i in range(1,5,2):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, shots=i)\n",
    "\n",
    "# fit data to OneClassSVM\n",
    "# svm.fit(precomp_kernel_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22168876",
   "metadata": {},
   "source": [
    "Comparison to other One-Class SVM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e8057e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For dataset: 1\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      1.00      0.67         3\n",
      "           1       1.00      0.92      0.96        39\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.75      0.96      0.81        42\n",
      "weighted avg       0.96      0.93      0.94        42\n",
      "\n",
      "Accuracy: 0.9285714285714286\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.5\n",
      "Recall 1: 0.9230769230769231\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.9600000000000001\n",
      "F1 -1: 0.6666666666666666\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision 1: 1.0\n",
      "Precision -1: 1.0\n",
      "Recall 1: 1.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 1.0\n",
      "F1 -1: 1.0\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision 1: 1.0\n",
      "Precision -1: 1.0\n",
      "Recall 1: 1.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 1.0\n",
      "F1 -1: 1.0\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.07      1.00      0.13         3\n",
      "           1       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.07        42\n",
      "   macro avg       0.04      0.50      0.07        42\n",
      "weighted avg       0.01      0.07      0.01        42\n",
      "\n",
      "Accuracy: 0.07142857142857142\n",
      "Precision 1: 0.0\n",
      "Precision -1: 0.07142857142857142\n",
      "Recall 1: 0.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.0\n",
      "F1 -1: 0.13333333333333333\n",
      "\n",
      "For dataset: 2\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.33      0.40         6\n",
      "           1       0.88      0.93      0.90        30\n",
      "\n",
      "    accuracy                           0.83        36\n",
      "   macro avg       0.69      0.63      0.65        36\n",
      "weighted avg       0.81      0.83      0.82        36\n",
      "\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision 1: 0.875\n",
      "Precision -1: 0.5\n",
      "Recall 1: 0.9333333333333333\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.9032258064516129\n",
      "F1 -1: 0.4\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "\n",
      "For dataset: 3\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         3\n",
      "           1       0.91      0.82      0.86        39\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.46      0.41      0.43        42\n",
      "weighted avg       0.85      0.76      0.80        42\n",
      "\n",
      "Accuracy: 0.7619047619047619\n",
      "Precision 1: 0.9142857142857143\n",
      "Precision -1: 0.0\n",
      "Recall 1: 0.8205128205128205\n",
      "Recall -1: 0.0\n",
      "F1 1: 0.8648648648648648\n",
      "F1 -1: 0.0\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.21      1.00      0.35         3\n",
      "           1       1.00      0.72      0.84        39\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.61      0.86      0.59        42\n",
      "weighted avg       0.94      0.74      0.80        42\n",
      "\n",
      "Accuracy: 0.7380952380952381\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.21428571428571427\n",
      "Recall 1: 0.717948717948718\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.835820895522388\n",
      "F1 -1: 0.35294117647058826\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.09      1.00      0.16         3\n",
      "           1       1.00      0.18      0.30        39\n",
      "\n",
      "    accuracy                           0.24        42\n",
      "   macro avg       0.54      0.59      0.23        42\n",
      "weighted avg       0.93      0.24      0.29        42\n",
      "\n",
      "Accuracy: 0.23809523809523808\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.08571428571428572\n",
      "Recall 1: 0.1794871794871795\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.30434782608695654\n",
      "F1 -1: 0.15789473684210528\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         3\n",
      "           1       0.92      0.90      0.91        39\n",
      "\n",
      "    accuracy                           0.83        42\n",
      "   macro avg       0.46      0.45      0.45        42\n",
      "weighted avg       0.86      0.83      0.84        42\n",
      "\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision 1: 0.9210526315789473\n",
      "Precision -1: 0.0\n",
      "Recall 1: 0.8974358974358975\n",
      "Recall -1: 0.0\n",
      "F1 1: 0.9090909090909091\n",
      "F1 -1: 0.0\n",
      "\n",
      "For dataset: 4\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "other_kernel_list = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "outliers_fraction=20/210\n",
    "dataset_count = 0\n",
    "supervised = True\n",
    "for X,y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print()\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    for kernel in other_kernel_list:\n",
    "        svm_classical = OneClassSVM(kernel = kernel, verbose=True,  nu=outliers_fraction)\n",
    "        if supervised:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "            svm_classical.fit(X_train, y_train)\n",
    "            y_pred = svm_classical.predict(X_test)\n",
    "            # TODO save to Matrix\n",
    "            print(\"{}: \".format(kernel))\n",
    "#             print(classification_report(y_test, y_pred))\n",
    "            print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "            \n",
    "            print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "            print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='macro')))\n",
    "            print(\"F1 1: {}\".format(f1_score(y, y_pred, average='macro')))\n",
    "            \n",
    "#             print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "#             print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "#             print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "#             print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        else:\n",
    "            svm_classical.fit(X)\n",
    "            y_pred = svm_classical.predict(X)\n",
    "            # TODO save to Matrix\n",
    "\n",
    "            print(\"{}: \".format(kernel))\n",
    "#             print(classification_report(y, y_pred))\n",
    "            print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "            \n",
    "            print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "            print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='macro')))\n",
    "            print(\"F1 1: {}\".format(f1_score(y, y_pred, average='macro')))\n",
    "#             print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "#             print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "#             print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "#             print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480e331",
   "metadata": {},
   "source": [
    "Second algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e969bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def K(x,z, p_constant=1.0):\n",
    "    return (np.dot(x.T,z)+p_constant)**2\n",
    "\n",
    "def Algorithm2(X, y, reps, shots, outliers_fraction,entanglement=\"linear\", num_features = 2, seed = 0, supervised=False):\n",
    "    # Qiskit code for RawFeatureVector has bug, currently not in use\n",
    "    # TODO We should use RawFeatureVector to stay true to the paper, but Qiskit currently has problem with this method\n",
    "#     feature_map = RawFeatureVector(feature_dimension = num_features)\n",
    "#     print(feature_map.parameters)\n",
    "#     par0 = feature_map.parameters[0]\n",
    "#     par1 = feature_map.parameters[1]\n",
    "#     print(par0)\n",
    "#     print(par1)\n",
    "#     state = np.array([1, 1]) / np.sqrt(2)\n",
    "#     feature_map = feature_map.assign_parameters(state)\n",
    "#     theta_range = np.linspace(0, 2 * np.pi, 128)\n",
    "#     print(feature_map.parameters)\n",
    "#     feature_map = feature_map.bind_parameters({par0: 1/np.sqrt(2), par1: 1/np.sqrt(2)})\n",
    "#     feature_map = ZZFeatureMap(feature_dimension = num_features, reps = reps, entanglement=entanglement)\n",
    "#     feature_map = ZFeatureMap(feature_dimension = num_features, reps = reps)\n",
    "#     feature_map = PauliFeatureMap(feature_dimension = num_features, reps = reps)\n",
    "\n",
    "#     # Calculates probabilities of bit results from quantum circuits\n",
    "#     sampler = Sampler()\n",
    "#     # uses sampler to calculate state fidelity of 2 quantum circuits\n",
    "#     fidelity = ComputeUncompute(sampler=sampler)\n",
    "#     # Translates data with base state fidelity distance metric\n",
    "#     kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "#     # Kernel needs to be evaluated before going into the One-Class SVM\n",
    "\n",
    "#     # Algorithm 2 starts here \n",
    "#     svm = OneClassSVM(kernel = kernel.evaluate, verbose=True, nu=outliers_fraction)\n",
    "# #     classical_solution = NumPyLinearSolver().solve(X, y / np.linalg.norm(y))\n",
    "\n",
    "    # Trick to make matrix hermitian\n",
    "#     X = np.matrix(X)\n",
    "#     Xh = X.getH()\n",
    "#     zeroes1 = np.zeros((X.shape[0], X.shape[0]))\n",
    "#     zeroes2 = np.zeros((X.shape[1], X.shape[1]))\n",
    "    \n",
    "#     X = np.bmat([[zeroes1, X], [Xh, zeroes2]])\n",
    "#     y = np.append(y, [1,1])\n",
    "#     X = DensityMatrix(X)\n",
    "#     np.real(X)\n",
    "#     print(X)\n",
    "#     hhl = HHL()\n",
    "#     hhl.construct_circuit(X,y)\n",
    "#     inversed_matrix = hhl.solve(X,y)\n",
    "\n",
    "    # Calculate Kernel Matrix\n",
    "    K = np.dot(X, np.transpose(X))\n",
    "    \n",
    "    # Change Kernel Matrix to density matrix gram\n",
    "    K_ = np.divide(K, K.trace())\n",
    "    \n",
    "    # density matrix exponentiation technique\n",
    "    # First make matrix hermitian\n",
    "    K_ = np.matrix(K_)\n",
    "    K_h = K_.getH()\n",
    "    zeroes1 = np.zeros((K_.shape[0], K_.shape[0]))\n",
    "    zeroes2 = np.zeros((K_.shape[1], K_.shape[1]))\n",
    "    \n",
    "    hermitian_K = np.bmat([[zeroes1, K_], [K_h, zeroes2]])\n",
    "#     y = np.append(y, [1,1])\n",
    "    \n",
    "    \n",
    "#     K_dense = DensityMatrix(K_)\n",
    "#     K_q = Operator(K_)\n",
    "#     print(K_q)\n",
    "#     K_q = K_q.to_instruction()\n",
    "    h = HamiltonianGate(hermitian_K, 1)\n",
    "    print(len(K_))\n",
    "    print(len(X))\n",
    "    return\n",
    "    # invert Kernel Matrix\n",
    "    hhl = HHL()\n",
    "    inversed_matrix = hhl.solve(K_,y)\n",
    "    \n",
    "    # Test with custom feature map\n",
    "    feature_map = PauliFeatureMap(feature_dimension=2, reps=reps, data_map_func=feature_map_superfidel)\n",
    "    # Calculates probabilities of bit results from quantum circuits\n",
    "    sampler = Sampler()\n",
    "    # uses sampler to calculate state fidelity of 2 quantum circuits\n",
    "    fidelity = ComputeUncompute(sampler=sampler)\n",
    "    # Translates data with base state fidelity distance metric\n",
    "    kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "    svm = OneClassSVM(kernel = kernel.evaluate, verbose=True, nu=outliers_fraction)\n",
    "\n",
    "    if supervised: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        # TODO save to Matrix\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "    else: \n",
    "        svm.fit(X)\n",
    "        y_pred = svm.predict(X)\n",
    "        #TODO save to matrix\n",
    "#         print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "        \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4ed2",
   "metadata": {},
   "source": [
    "Second algorithm attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d237a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset: 1\n",
      "For reps: 2\n",
      "210\n",
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slang\\AppData\\Local\\Temp/ipykernel_2268/2593100918.py:95: DeprecationWarning: The HHL class is deprecated as of Qiskit Terra 0.22.0 and will be removed\n",
      "        no sooner than 3 months after the release date.\n",
      "        It is replaced by the tutorial at https://qiskit.org/textbook/ch-applications/hhl_tutorial.html\"\n",
      "        \n",
      "  hhl = HHL()\n"
     ]
    },
    {
     "ename": "QiskitError",
     "evalue": "'The number of rows of the isometry is not a non negative power of 2.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mQiskitError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2268/3338012895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For reps: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mAlgorithm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutliers_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutliers_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2268/2593100918.py\u001b[0m in \u001b[0;36mAlgorithm2\u001b[1;34m(X, y, reps, shots, outliers_fraction, entanglement, num_features, seed, supervised)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m# invert Kernel Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mhhl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHHL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0minversed_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhhl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# Test with custom feature map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\qiskit\\algorithms\\linear_solvers\\hhl.py\u001b[0m in \u001b[0;36msolve\u001b[1;34m(self, matrix, vector, observable, observable_circuit, post_processing)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0msolution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearSolverResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0msolution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_circuit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[0msolution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meuclidean_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calculate_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\qiskit\\algorithms\\linear_solvers\\hhl.py\u001b[0m in \u001b[0;36mconstruct_circuit\u001b[1;34m(self, matrix, vector, neg_vals)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             \u001b[0mvector_circuit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQuantumCircuit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mvector_circuit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# If state preparation is probabilistic the number of qubit flags should increase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\qiskit\\extensions\\quantum_initializer\\isometry.py\u001b[0m in \u001b[0;36miso\u001b[1;34m(self, isometry, q_input, q_ancillas_for_output, q_ancillas_zero, q_ancillas_dirty, epsilon)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     return self.append(\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0mIsometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misometry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_ancillas_zero\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_ancillas_dirty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m         \u001b[0mq_input\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mq_ancillas_for_output\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mq_ancillas_zero\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mq_ancillas_dirty\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\qiskit\\extensions\\quantum_initializer\\isometry.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, isometry, num_ancillas_zero, num_ancillas_dirty, epsilon)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misometry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             raise QiskitError(\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;34m\"The number of rows of the isometry is not a non negative power of 2.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             )\n",
      "\u001b[1;31mQiskitError\u001b[0m: 'The number of rows of the isometry is not a non negative power of 2.'"
     ]
    }
   ],
   "source": [
    "dataset_count = 0\n",
    "outliers_fraction=20/210\n",
    "\n",
    "for X, y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    for i in range(2,3):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm2(X, y, i, 1, outliers_fraction=outliers_fraction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8e8b0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\slang'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b8009d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

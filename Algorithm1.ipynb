{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc664edf",
   "metadata": {},
   "source": [
    "ZZ Feature Map Algorithm 1 test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de16c4",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d659b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import BasicAer\n",
    "from qiskit import Aer, transpile\n",
    "\n",
    "from qiskit.utils import QuantumInstance\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.circuit.library import PauliFeatureMap\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit.circuit.library import StatePreparation\n",
    "\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "import qiskit_machine_learning.kernels\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC, OneClassSVM\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from qiskit.providers.aer import AerError\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "from qiskit.algorithms.state_fidelities import ComputeUncompute\n",
    "algorithm_globals.random_seed = 0\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "\n",
    "dataset_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4a5f750",
   "metadata": {},
   "source": [
    "Code used to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4925e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(13)\n",
    "# seed = 0\n",
    "# x, y = make_blobs(n_samples=190, centers=1, cluster_std=.1, center_box=(1.4, 1.4), random_state=seed)\n",
    "\n",
    "# out1x, out1y = make_blobs(n_samples=9, centers=1, cluster_std=.1, center_box=(1, 1), random_state=seed)\n",
    "# out2x, out2y = make_blobs(n_samples=11, centers=1, cluster_std=.1, center_box=(3.2, 0.01), random_state=seed)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# y[y == 0] = 1\n",
    "# out1y[out1y==0] = -1 \n",
    "# out2y[out2y==0] = -1 \n",
    "\n",
    "# x = np.append(x, out1x, axis = 0)\n",
    "# y = np.append(y, out1y, axis = 0)\n",
    "\n",
    "# x = np.append(x, out2x, axis = 0)\n",
    "# y = np.append(y, out2y, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(x[:,0], x[:,1])\n",
    "# plt.scatter(out1x[:,0], out1x[:,1])\n",
    "# plt.scatter(out2x[:,0], out2x[:,1])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626d6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second dataset: ZZFeatureMap ad_hoc data\n",
    "# adhoc_dimension = 2\n",
    "# train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "#     training_size=80,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.3,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=True,\n",
    "# )\n",
    "# train_features_out, train_labels_out, _, _ = ad_hoc_data(\n",
    "#     training_size=10,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.6,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "# # adhoc_total[adhoc_total == 0] = 1\n",
    "\n",
    "# # print(train_features)\n",
    "# # print(train_labels)\n",
    "\n",
    "# # Change labels for One-Class\n",
    "# train_labels_out[train_labels_out != -1] = -1\n",
    "\n",
    "# # Now we have to add the outliers\n",
    "# train_features = np.append(train_features, train_features_out, axis = 0)\n",
    "# train_labels = np.append(train_labels, train_labels_out, axis = 0)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.ylim(0, 2 * np.pi)\n",
    "# plt.xlim(0, 2 * np.pi)\n",
    "# plt.imshow(\n",
    "#     np.asmatrix(adhoc_total).T,\n",
    "#     interpolation=\"nearest\",\n",
    "#     origin=\"lower\",\n",
    "#     cmap=\"RdBu\",\n",
    "#     extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    "# )\n",
    "\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 0), 0],\n",
    "#     train_features[np.where(train_labels[:] == 0), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"b\",\n",
    "#     label=\"Training Label A\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 1), 0],\n",
    "#     train_features[np.where(train_labels[:] == 1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "#     label=\"Training Label B\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == -1), 0],\n",
    "#     train_features[np.where(train_labels[:] == -1), 1],\n",
    "#     marker=\"s\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"k\",\n",
    "#     label=\"Outliers\",\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "# plt.title(\"Ad hoc dataset for classification\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # We don't need the second label for one-class SVMs, so change 0 label to 1\n",
    "# train_labels[train_labels == 0] = 1\n",
    "\n",
    "\n",
    "# # add data to dataset_list\n",
    "# dataset_list.append([train_features,train_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e8ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Third dataset, create blob and add zz feature map data to it\n",
    "# random.seed(13)\n",
    "# seed = 23\n",
    "# x, y = make_blobs(n_samples=190, centers=1, cluster_std=3, center_box=(3, 3), random_state=seed)\n",
    "# train_features_out2, train_labels_out2, _, _ = ad_hoc_data(\n",
    "#     training_size=10,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.5,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# y[y == 0] = 1\n",
    "# train_labels_out2[train_labels_out2 != -1] = -1 \n",
    "\n",
    "# # add outliers to data\n",
    "# x = np.append(x, train_features_out2, axis = 0)\n",
    "# y = np.append(y, train_labels_out2, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(x[:,0], x[:,1])\n",
    "# plt.scatter(\n",
    "#     train_features_out2[np.where(train_labels_out2[:] == -1), 0],\n",
    "#     train_features_out2[np.where(train_labels_out2[:] == -1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "#     label=\"Training Label B\",\n",
    "# )\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([x,y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f41d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adhoc_dimension = 2\n",
    "# train_features, train_labels, _,_ = ad_hoc_data(\n",
    "#     training_size=80,\n",
    "#     test_size=0,\n",
    "#     n=adhoc_dimension,\n",
    "#     gap=0.6,\n",
    "#     plot_data=False,\n",
    "#     one_hot=False,\n",
    "#     include_sample_total=False,\n",
    "# )\n",
    "# x, y = make_blobs(n_samples=20, centers=3, cluster_std=2, center_box=(3, 3), random_state=111)\n",
    "\n",
    "# # prepare data for One-Class model\n",
    "# train_labels[train_labels == 0] = 1\n",
    "# y[y != -1] = -1 \n",
    "\n",
    "# # add outliers to data\n",
    "# train_features = np.append(train_features, x, axis = 0)\n",
    "# train_labels = np.append(train_labels, y, axis = 0)\n",
    "\n",
    "# # Plot to see data\n",
    "# plt.scatter(\n",
    "#     train_features[np.where(train_labels[:] == 1), 0],\n",
    "#     train_features[np.where(train_labels[:] == 1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"b\",\n",
    "#     label=\"Training Label A\",\n",
    "# )\n",
    "# plt.scatter(\n",
    "#     x[np.where(y[:] == -1), 0],\n",
    "#     x[np.where(y[:] == -1), 1],\n",
    "#     marker=\"o\",\n",
    "#     facecolors=\"w\",\n",
    "#     edgecolors=\"r\",\n",
    "# )\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Add data to dataset_list\n",
    "# dataset_list.append([train_features,train_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a8ae",
   "metadata": {},
   "source": [
    "Save/Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d2a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as fileload:\n",
    "        file = pickle.load(fileload)\n",
    "    return file\n",
    "# save_object(dataset_list, \"datasets\")\n",
    "dataset_list = load_object(\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e32a7",
   "metadata": {},
   "source": [
    "Quantum Function for OneClass (Algorithm 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58ba946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First algorithm, returns trained model\n",
    "def Algorithm1(X, y, reps, shots, outliers_fraction,entanglement=\"linear\", num_features = 2, seed = 0, supervised=False):\n",
    "    # Define ZZFeatureMap using inputs\n",
    "    feature_map = ZZFeatureMap(feature_dimension = num_features, reps = reps, entanglement=entanglement)\n",
    "    # Calculates probabilities of bit results from quantum circuits\n",
    "    sampler = Sampler()\n",
    "    # uses sampler to calculate state fidelity of 2 quantum circuits\n",
    "    fidelity = ComputeUncompute(sampler=sampler)\n",
    "    # Translates data with base state fidelity distance metric\n",
    "    kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "    # Kernel needs to be evaluated before going into the One-Class SVM\n",
    "    svm = OneClassSVM(kernel = kernel.evaluate, verbose=True, nu=outliers_fraction)\n",
    "    if supervised: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        # TODO save to Matrix\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "    else: \n",
    "        svm.fit(X)\n",
    "        y_pred = svm.predict(X)\n",
    "        #TODO save to matrix\n",
    "#         print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0a968",
   "metadata": {},
   "source": [
    "Showcase of Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1bba916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For dataset: 1\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.60      0.60        20\n",
      "           1       0.96      0.96      0.96       190\n",
      "\n",
      "    accuracy                           0.92       210\n",
      "   macro avg       0.78      0.78      0.78       210\n",
      "weighted avg       0.92      0.92      0.92       210\n",
      "\n",
      "Accuracy: 0.9238095238095239\n",
      "Precision 1: 0.9578947368421052\n",
      "Precision -1: 0.6\n",
      "Recall 1: 0.9578947368421052\n",
      "Recall -1: 0.6\n",
      "F1 1: 0.9578947368421052\n",
      "F1 -1: 0.6\n",
      "\n",
      "For dataset: 2\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.09      0.20      0.13        20\n",
      "           1       0.88      0.76      0.81       160\n",
      "\n",
      "    accuracy                           0.69       180\n",
      "   macro avg       0.49      0.48      0.47       180\n",
      "weighted avg       0.80      0.69      0.74       180\n",
      "\n",
      "Accuracy: 0.6944444444444444\n",
      "Precision 1: 0.8832116788321168\n",
      "Precision -1: 0.09302325581395349\n",
      "Recall 1: 0.75625\n",
      "Recall -1: 0.2\n",
      "F1 1: 0.8148148148148148\n",
      "F1 -1: 0.12698412698412698\n",
      "\n",
      "For dataset: 3\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.16      0.50      0.24        20\n",
      "           1       0.93      0.72      0.81       190\n",
      "\n",
      "    accuracy                           0.70       210\n",
      "   macro avg       0.54      0.61      0.52       210\n",
      "weighted avg       0.86      0.70      0.76       210\n",
      "\n",
      "Accuracy: 0.6952380952380952\n",
      "Precision 1: 0.9315068493150684\n",
      "Precision -1: 0.15625\n",
      "Recall 1: 0.7157894736842105\n",
      "Recall -1: 0.5\n",
      "F1 1: 0.8095238095238095\n",
      "F1 -1: 0.23809523809523808\n",
      "\n",
      "For dataset: 4\n",
      "For reps: 2\n",
      "[LibSVM]              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.11      0.20      0.15        20\n",
      "           1       0.89      0.81      0.85       160\n",
      "\n",
      "    accuracy                           0.74       180\n",
      "   macro avg       0.50      0.50      0.50       180\n",
      "weighted avg       0.80      0.74      0.77       180\n",
      "\n",
      "Accuracy: 0.7388888888888889\n",
      "Precision 1: 0.8896551724137931\n",
      "Precision -1: 0.11428571428571428\n",
      "Recall 1: 0.80625\n",
      "Recall -1: 0.2\n",
      "F1 1: 0.8459016393442622\n",
      "F1 -1: 0.14545454545454545\n"
     ]
    }
   ],
   "source": [
    "dataset_count = 0\n",
    "outliers_fraction=20/210\n",
    "for X, y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print()\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    for i in range(2,3):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "        Algorithm1(X, y, i, 1, outliers_fraction=outliers_fraction)\n",
    "\n",
    "# fit data to OneClassSVM\n",
    "# svm.fit(precomp_kernel_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22168876",
   "metadata": {},
   "source": [
    "Comparison to other One-Class SVM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21e8057e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For dataset: 1\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      1.00      0.67         3\n",
      "           1       1.00      0.92      0.96        39\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.75      0.96      0.81        42\n",
      "weighted avg       0.96      0.93      0.94        42\n",
      "\n",
      "Accuracy: 0.9285714285714286\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.5\n",
      "Recall 1: 0.9230769230769231\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.9600000000000001\n",
      "F1 -1: 0.6666666666666666\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision 1: 1.0\n",
      "Precision -1: 1.0\n",
      "Recall 1: 1.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 1.0\n",
      "F1 -1: 1.0\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           1.00        42\n",
      "   macro avg       1.00      1.00      1.00        42\n",
      "weighted avg       1.00      1.00      1.00        42\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision 1: 1.0\n",
      "Precision -1: 1.0\n",
      "Recall 1: 1.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 1.0\n",
      "F1 -1: 1.0\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.07      1.00      0.13         3\n",
      "           1       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.07        42\n",
      "   macro avg       0.04      0.50      0.07        42\n",
      "weighted avg       0.01      0.07      0.01        42\n",
      "\n",
      "Accuracy: 0.07142857142857142\n",
      "Precision 1: 0.0\n",
      "Precision -1: 0.07142857142857142\n",
      "Recall 1: 0.0\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.0\n",
      "F1 -1: 0.13333333333333333\n",
      "\n",
      "For dataset: 2\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.50      0.33      0.40         6\n",
      "           1       0.88      0.93      0.90        30\n",
      "\n",
      "    accuracy                           0.83        36\n",
      "   macro avg       0.69      0.63      0.65        36\n",
      "weighted avg       0.81      0.83      0.82        36\n",
      "\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision 1: 0.875\n",
      "Precision -1: 0.5\n",
      "Recall 1: 0.9333333333333333\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.9032258064516129\n",
      "F1 -1: 0.4\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "\n",
      "For dataset: 3\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         3\n",
      "           1       0.91      0.82      0.86        39\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.46      0.41      0.43        42\n",
      "weighted avg       0.85      0.76      0.80        42\n",
      "\n",
      "Accuracy: 0.7619047619047619\n",
      "Precision 1: 0.9142857142857143\n",
      "Precision -1: 0.0\n",
      "Recall 1: 0.8205128205128205\n",
      "Recall -1: 0.0\n",
      "F1 1: 0.8648648648648648\n",
      "F1 -1: 0.0\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.21      1.00      0.35         3\n",
      "           1       1.00      0.72      0.84        39\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.61      0.86      0.59        42\n",
      "weighted avg       0.94      0.74      0.80        42\n",
      "\n",
      "Accuracy: 0.7380952380952381\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.21428571428571427\n",
      "Recall 1: 0.717948717948718\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.835820895522388\n",
      "F1 -1: 0.35294117647058826\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.09      1.00      0.16         3\n",
      "           1       1.00      0.18      0.30        39\n",
      "\n",
      "    accuracy                           0.24        42\n",
      "   macro avg       0.54      0.59      0.23        42\n",
      "weighted avg       0.93      0.24      0.29        42\n",
      "\n",
      "Accuracy: 0.23809523809523808\n",
      "Precision 1: 1.0\n",
      "Precision -1: 0.08571428571428572\n",
      "Recall 1: 0.1794871794871795\n",
      "Recall -1: 1.0\n",
      "F1 1: 0.30434782608695654\n",
      "F1 -1: 0.15789473684210528\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         3\n",
      "           1       0.92      0.90      0.91        39\n",
      "\n",
      "    accuracy                           0.83        42\n",
      "   macro avg       0.46      0.45      0.45        42\n",
      "weighted avg       0.86      0.83      0.84        42\n",
      "\n",
      "Accuracy: 0.8333333333333334\n",
      "Precision 1: 0.9210526315789473\n",
      "Precision -1: 0.0\n",
      "Recall 1: 0.8974358974358975\n",
      "Recall -1: 0.0\n",
      "F1 1: 0.9090909090909091\n",
      "F1 -1: 0.0\n",
      "\n",
      "For dataset: 4\n",
      "[LibSVM]rbf: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.33      0.36         6\n",
      "           1       0.87      0.90      0.89        30\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.64      0.62      0.62        36\n",
      "weighted avg       0.79      0.81      0.80        36\n",
      "\n",
      "Accuracy: 0.8055555555555556\n",
      "Precision 1: 0.8709677419354839\n",
      "Precision -1: 0.4\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.3333333333333333\n",
      "F1 1: 0.8852459016393444\n",
      "F1 -1: 0.3636363636363636\n",
      "[LibSVM]linear: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n",
      "[LibSVM]poly: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n",
      "[LibSVM]sigmoid: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.25      0.17      0.20         6\n",
      "           1       0.84      0.90      0.87        30\n",
      "\n",
      "    accuracy                           0.78        36\n",
      "   macro avg       0.55      0.53      0.54        36\n",
      "weighted avg       0.74      0.78      0.76        36\n",
      "\n",
      "Accuracy: 0.7777777777777778\n",
      "Precision 1: 0.84375\n",
      "Precision -1: 0.25\n",
      "Recall 1: 0.9\n",
      "Recall -1: 0.16666666666666666\n",
      "F1 1: 0.870967741935484\n",
      "F1 -1: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\slang\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "other_kernel_list = ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "outliers_fraction=20/210\n",
    "dataset_count = 0\n",
    "supervised = True\n",
    "for X,y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print()\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    for kernel in other_kernel_list:\n",
    "        svm_classical = OneClassSVM(kernel = kernel, verbose=True,  nu=outliers_fraction)\n",
    "        if supervised:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "            svm_classical.fit(X_train, y_train)\n",
    "            y_pred = svm_classical.predict(X_test)\n",
    "            # TODO save to Matrix\n",
    "            print(\"{}: \".format(kernel))\n",
    "#             print(classification_report(y_test, y_pred))\n",
    "            print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "            \n",
    "            print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "            print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "            print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "            print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        else:\n",
    "            svm_classical.fit(X)\n",
    "            y_pred = svm_classical.predict(X)\n",
    "            # TODO save to Matrix\n",
    "\n",
    "            print(\"{}: \".format(kernel))\n",
    "#             print(classification_report(y, y_pred))\n",
    "            print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "            \n",
    "            print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "            print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "            print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "            \n",
    "            print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "            print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4480e331",
   "metadata": {},
   "source": [
    "Second algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9e969bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.algorithms.linear_solvers.numpy_linear_solver import NumPyLinearSolver\n",
    "from qiskit.algorithms.linear_solvers.hhl import HHL\n",
    "from qiskit.quantum_info import DensityMatrix\n",
    "from sympy import Matrix\n",
    "def K(x,z, p_constant=1.0):\n",
    "    return (np.dot(x.T,z)+p_constant)**2\n",
    "def Algorithm2(X, y, reps, shots, outliers_fraction,entanglement=\"linear\", num_features = 2, seed = 0, supervised=False):\n",
    "    # Qiskit code for RawFeatureVector has bug, currently not in use\n",
    "    # TODO We should use RawFeatureVector to stay true to the paper, but Qiskit currently has problem with this method\n",
    "#     feature_map = RawFeatureVector(feature_dimension = num_features)\n",
    "#     print(feature_map.parameters)\n",
    "#     par0 = feature_map.parameters[0]\n",
    "#     par1 = feature_map.parameters[1]\n",
    "#     print(par0)\n",
    "#     print(par1)\n",
    "#     state = np.array([1, 1]) / np.sqrt(2)\n",
    "#     feature_map = feature_map.assign_parameters(state)\n",
    "#     theta_range = np.linspace(0, 2 * np.pi, 128)\n",
    "#     print(feature_map.parameters)\n",
    "#     feature_map = feature_map.bind_parameters({par0: 1/np.sqrt(2), par1: 1/np.sqrt(2)})\n",
    "#     feature_map = ZZFeatureMap(feature_dimension = num_features, reps = reps, entanglement=entanglement)\n",
    "#     feature_map = ZFeatureMap(feature_dimension = num_features, reps = reps)\n",
    "#     feature_map = PauliFeatureMap(feature_dimension = num_features, reps = reps)\n",
    "\n",
    "#     # Calculates probabilities of bit results from quantum circuits\n",
    "#     sampler = Sampler()\n",
    "#     # uses sampler to calculate state fidelity of 2 quantum circuits\n",
    "#     fidelity = ComputeUncompute(sampler=sampler)\n",
    "#     # Translates data with base state fidelity distance metric\n",
    "#     kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)\n",
    "#     # Kernel needs to be evaluated before going into the One-Class SVM\n",
    "\n",
    "#     # Algorithm 2 starts here \n",
    "#     svm = OneClassSVM(kernel = kernel.evaluate, verbose=True, nu=outliers_fraction)\n",
    "# #     classical_solution = NumPyLinearSolver().solve(X, y / np.linalg.norm(y))\n",
    "\n",
    "    # Trick to make matrix hermitian\n",
    "#     X = np.matrix(X)\n",
    "#     Xh = X.getH()\n",
    "#     zeroes1 = np.zeros((X.shape[0], X.shape[0]))\n",
    "#     zeroes2 = np.zeros((X.shape[1], X.shape[1]))\n",
    "    \n",
    "#     X = np.bmat([[zeroes1, X], [Xh, zeroes2]])\n",
    "#     y = np.append(y, [1,1])\n",
    "#     X = DensityMatrix(X)\n",
    "#     np.real(X)\n",
    "#     print(X)\n",
    "#     hhl = HHL()\n",
    "#     hhl.construct_circuit(X,y)\n",
    "#     inversed_matrix = hhl.solve(X,y)\n",
    "\n",
    "    # STEP 1 Calculate Kernel Matrix\n",
    "    K = np.dot(X, np.transpose(X))\n",
    "    # STEP 2 idk man\n",
    "    svm = OneClassSVM(kernel = 'precomputed', verbose=True, nu=outliers_fraction)\n",
    "\n",
    "    if supervised: \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "        svm.fit(X_train, y_train)\n",
    "        y_pred = svm.predict(X_test)\n",
    "        # TODO save to Matrix\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y_test, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y_test, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y_test, y_pred, pos_label=-1, average='binary')))\n",
    "    else: \n",
    "        svm.fit(X)\n",
    "        y_pred = svm.predict(X)\n",
    "        #TODO save to matrix\n",
    "#         print(classification_report(y, y_pred))\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(y, y_pred)))\n",
    "\n",
    "        print(\"Precision 1: {}\".format(precision_score(y, y_pred, average='binary')))\n",
    "        print(\"Precision -1: {}\".format(precision_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"Recall 1: {}\".format(recall_score(y, y_pred, average='binary')))\n",
    "        print(\"Recall -1: {}\".format(recall_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "\n",
    "        print(\"F1 1: {}\".format(f1_score(y, y_pred, average='binary')))\n",
    "        print(\"F1 -1: {}\".format(f1_score(y, y_pred, pos_label=-1, average='binary')))\n",
    "        \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4ed2",
   "metadata": {},
   "source": [
    "Second algorithm attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d237a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset: 1\n",
      "For reps: 2\n",
      "[[ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [ 0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]\n",
      " [-0.06900656+0.j]]\n",
      "7.714245517666122\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slang\\AppData\\Local\\Temp/ipykernel_15872/946763017.py:54: DeprecationWarning: The HHL class is deprecated as of Qiskit Terra 0.22.0 and will be removed\n",
      "        no sooner than 3 months after the release date.\n",
      "        It is replaced by the tutorial at https://qiskit.org/textbook/ch-applications/hhl_tutorial.html\"\n",
      "        \n",
      "  hhl = HHL()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input matrix dimension must be 2^n!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15872/2990155540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#         X = squarify(X,0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mAlgorithm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutliers_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutliers_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15872/946763017.py\u001b[0m in \u001b[0;36mAlgorithm2\u001b[1;34m(X, y, reps, shots, outliers_fraction, entanglement, num_features, seed, supervised)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mhhl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHHL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mhhl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_circuit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;31m# STEP 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0msvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneClassSVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'precomputed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutliers_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\qiskit\\algorithms\\linear_solvers\\hhl.py\u001b[0m in \u001b[0;36mconstruct_circuit\u001b[1;34m(self, matrix, vector, neg_vals)\u001b[0m\n\u001b[0;32m    370\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input matrix must be square!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input matrix dimension must be 2^n!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input matrix must be hermitian!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input matrix dimension must be 2^n!"
     ]
    }
   ],
   "source": [
    "# def squarify(M,val):\n",
    "#     print(M.shape)\n",
    "#     (a,b)=M.shape\n",
    "#     if a>b:\n",
    "#         padding=((0,0),(0,a-b))\n",
    "#     else:\n",
    "#         padding=((0,b-a),(0,0))\n",
    "#     return np.pad(M,padding,mode='constant',constant_values=val)\n",
    "dataset_count = 0\n",
    "outliers_fraction=20/210\n",
    "for X, y in dataset_list:\n",
    "    dataset_count = dataset_count + 1\n",
    "    print(\"For dataset: {}\".format(dataset_count))\n",
    "    for i in range(2,3):\n",
    "        print(\"For reps: {}\".format(i))\n",
    "#         X = X[:128]\n",
    "#         y = y[:128]\n",
    "#         print(X.shape)\n",
    "#         X = squarify(X,0)\n",
    "\n",
    "        Algorithm2(X, y, i, 1, outliers_fraction=outliers_fraction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e8b0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\slang'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
